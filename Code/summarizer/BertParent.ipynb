{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMohIuHEDndTaMxaSCcKAyt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"46DzDU3f0D-R"},"outputs":[],"source":["from pytorch_pretrained_bert import BertTokenizer, BertModel, GPT2Model, GPT2Tokenizer\n","import logging\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from numpy import ndarray\n","from typing import List\n","\n","logging.basicConfig(level=logging.WARNING)\n","\n","\n","class BertParent(object):\n","\n","    def __init__(self, model: str, vector_size: int=None):\n","        self.model = BertModel.from_pretrained(model)\n","        self.tokenizer = BertTokenizer.from_pretrained(model)\n","\n","        if model == 'bert-base-uncased':\n","            self.vector_size = 768\n","        elif model == 'bert-large-uncased':\n","            self.vector_size = 1024\n","        elif vector_size is None:\n","            raise RuntimeError(\"Vector size must be supplied for custom models\")\n","        else:\n","            self.vector_size = vector_size\n","\n","        self.model.eval()\n","\n","    def tokenize_input(self, text: str) -> torch.tensor:\n","        tokenized_text = self.tokenizer.tokenize(text)\n","        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n","        return torch.tensor([indexed_tokens])\n","\n","    def extract_embeddings(self, text: str, hidden: int=-2, squeeze: bool=False, reduce_option: str ='mean') -> ndarray:\n","        tokens_tensor = self.tokenize_input(text)\n","        hidden_states, pooled = self.model(tokens_tensor)\n","\n","        if hidden < -1 and hidden > -12:\n","            if reduce_option == 'max':\n","                pooled = hidden_states[hidden].max(dim=1)\n","            elif reduce_option == 'median':\n","                pooled = hidden_states[hidden].median(dim=1)\n","            else:\n","                pooled = hidden_states[hidden].mean(dim=1)\n","\n","        if squeeze:\n","            return pooled.detach().numpy().squeeze()\n","\n","        return pooled\n","\n","    def create_matrix(self, content: List[str], hidden: int=-2, reduce_option: str = 'mean') -> ndarray:\n","        train_vec = np.zeros((len(content), self.vector_size))\n","        for i, t in tqdm(enumerate(content)):\n","            train_vec[i] = self.extract_embeddings(t, hidden=hidden, reduce_option=reduce_option).data.numpy()\n","        return train_vec\n","\n","    def __call__(self, content: List[str], hidden: int=-2, reduce_option: str = 'mean') -> ndarray:\n","        return self.create_matrix(content, hidden, reduce_option)\n"]}]}