# -*- coding: utf-8 -*-
"""summarize.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y4HxABEY_nyJrfR-6zG015fyu-Z0zakJ
"""

import numpy as np
import heapq
import logging
import nltk
import argparse
import re
import os
from summarizer import SingleModel


def process_documents_bert(model):
    path = os.getcwd()+'/Data/content'
    categories = os.listdir(path)
    for category in categories:
        category_path = path+'/'+category
        # Check whether a path pointing to a file
        if os.path.isfile(category_path) == False:
            documents = os.listdir(category_path)
            for document in documents:
                summarize_document_bert(model, document, category_path)


def summarize_document_bert(model, doc, path):
    # print(model,doc,path)
    document = path+'/'+doc
    # reading in text file

    with open(document, 'r') as d:
        text_data = d.read()
    if os.path.getsize(document) == 0 or len(text_data) == 0:
        summary = "File was empty!"
    else:
        # Passing full text to model
        summary = model(text_data)

    # creating final summary with a ratio of 0.13
    summary_file = '\n\nSUMMARY:\n' + summary

    folders = document.split('/')
    filepath = folders[-2]+'/'+folders[-1]

    with open('Data/summaries/'+filepath, 'w') as summary_output:
        for line in summary_file:
            summary_output.write(line)


def process_documents_vanilla():
    path = os.getcwd()+'/Data/content'
    categories = os.listdir(path)
    for category in categories:
        category_path = path+'/'+category
        # Check whether a path pointing to a file
        if os.path.isfile(category_path) == False:
            documents = os.listdir(category_path)
            for document in documents:
                summarize_document_vanilla(document, category_path)


def summarize_document_vanilla(doc, path):
    document = path+'/'+doc

    # reading in text file
    with open(document, 'r') as d:
        text_data = d.read()

    # text clean up
    text_data = re.sub(r'\[[0-9]*\]', ' ', text_data)
    text_data = re.sub(r'\s+', ' ', text_data)

    processed_article = re.sub('[^a-zA-Z]', ' ', text_data)
    processed_article = re.sub(r'\s+', ' ', processed_article)

    # sentence-level tokenization of full text
    sentence_list = nltk.sent_tokenize(text_data)

    # NLTK stopword list
    stopwords = nltk.corpus.stopwords.words('english')

    # creating term frequency dict
    word_frequencies = {}
    for word in nltk.word_tokenize(processed_article):
        if word not in stopwords:
            if word not in word_frequencies.keys():
                word_frequencies[word] = 1
            else:
                word_frequencies[word] += 1

    maximum_frequency = max(word_frequencies.values())

    # adding term frequency ratios as dict values
    for word in word_frequencies.keys():
        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

    # ranking sentences for summary inclusion
    sentence_scores = {}
    for sent in sentence_list:
        for word in nltk.word_tokenize(sent.lower()):
            if word in word_frequencies.keys():
                if len(sent.split(' ')) < 30:
                    if sent not in sentence_scores.keys():
                        sentence_scores[sent] = word_frequencies[word]
                    else:
                        sentence_scores[sent] += word_frequencies[word]

    # creating final summary with default 4 highest-scoring sentences
    summary_sentences = heapq.nlargest(
        4, sentence_scores, key=sentence_scores.get)
    summary_sentences = ''.join(summary_sentences)
    summary_file = '\n\nSUMMARY:\n' + summary_sentences

    folders = document.split('/')
    filepath = folders[-2]+'/'+folders[-1]

    with open('Data/summaries/'+filepath, 'w') as summary_output:
        for line in summary_file:
            summary_output.write(line)


def test():

    from google.colab import drive
    drive.mount('/content/drive')

#   cd /content/drive/My Drive/Colab Notebooks/summarization-bert-vs-baseline

#   !pip install -r requirements.txt

    from summarizer import SingleModel
    import os
    summarize_document(model, '001.txt', os.getcwd()+'/Data/content/users')
    model = SingleModel()
    process_documents(model)


# choose BERT or vanilla summarizer
choose_bert_or_vanilla = input(
    'Please enter 1 to use the BERT summarizer or 2 for the Vanilla summarizer:\n')

# BERT summarizer
if choose_bert_or_vanilla == '1':
    print('Welcome to the BERT Summarizer!\n')
    model = SingleModel()
    process_documents_bert(model)
    print('Processing finished...!')

# Vanilla summarizer
elif choose_bert_or_vanilla == '2':
    print('Welcome to the Vanilla Summarizer!\n')
    process_documents_vanilla()
    print('Processing finished...!')
else:
    print('\nMust choose from 1 or 2')
