{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOyDSmmJ6Aj49zVoaG15IWq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":21,"metadata":{"id":"UBhj4GbC_WnN","executionInfo":{"status":"error","timestamp":1669264226198,"user_tz":480,"elapsed":173,"user":{"displayName":"Sneha Bandi","userId":"00027953615612399574"}},"colab":{"base_uri":"https://localhost:8080/","height":393},"outputId":"e75b5906-76d2-4986-b4ce-59748b34d48c"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0b79552b5f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_processors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModelSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'summarizer'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# summarize.py\n","\n","# -*- coding: utf-8 -*-\n","\"\"\"summarize.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1y4HxABEY_nyJrfR-6zG015fyu-Z0zakJ\n","\"\"\"\n","\n","import numpy as np\n","import heapq\n","import logging\n","import nltk\n","import argparse\n","import re\n","import os\n","from summarizer.model_processors import SingleModel\n","\n","class ModelSelector(object):\n","    # init method or constructor\n","    def __init__(self):\n","    # choose BERT or vanilla summarizer\n","        choose_bert_or_vanilla = '1'\n","        # choose_bert_or_vanilla = input('Please enter 1 to use the BERT summarizer or 2 for the Vanilla summarizer:\\n')\n","\n","        # BERT summarizer\n","        if choose_bert_or_vanilla == '1':\n","            print('Welcome to the BERT Summarizer!\\n')\n","            self.process_documents(\"bert\")\n","            print('Processing finished...!')\n","\n","        # Vanilla summarizer\n","        elif choose_bert_or_vanilla == '2':\n","            print('Welcome to the Vanilla Summarizer!\\n')\n","            self.process_documents(\"vanilla\")\n","            print('Processing finished...!')\n","        else:\n","            print('\\nMust choose from 1 or 2')\n","\n","    def process_documents(self,model_type):\n","        path = os.getcwd()+'/../Data/summarized_data/content'\n","        categories = os.listdir(path)\n","        for category in categories:\n","            category_path = path+'/'+category\n","            # Check whether a path pointing to a file\n","            if os.path.isfile(category_path) == False:\n","                documents = os.listdir(category_path)\n","                for document in documents:\n","                    self.summarize_text(model_type, document, category_path)\n","\n","\n","    def summarize_text(self, model_type, doc, path):\n","        # print(model,doc,path)\n","        document = path+'/'+doc\n","        # reading in text file\n","\n","        with open(document, 'r') as d:\n","            text_data = d.read()\n","\n","        if os.path.getsize(document) == 0 or len(text_data) == 0:\n","            summary = \"File was empty!\"\n","        else:\n","            # Passing full text to model\n","            if model_type == \"bert\":\n","                model = SingleModel()\n","                summary = model(text_data)\n","            else:\n","                summary = self.vanilla(text_data)\n","\n","\n","        # creating final summary with a ratio of 0.13\n","        summary_file = '\\n\\nSUMMARY:\\n' + summary\n","\n","        folders = document.split('/')\n","        filepath = folders[-2]+'/'+folders[-1]\n","        write_path = os.getcwd()+'/../Data/summarized_data/summaries/'+filepath\n","        with open(write_path, 'w+') as summary_output:\n","            for line in summary_file:\n","                summary_output.write(line)\n","\n","\n","    def vanilla(self,text_data):\n","\n","        # text clean up\n","        text_data = re.sub(r'\\[[0-9]*\\]', ' ', text_data)\n","        text_data = re.sub(r'\\s+', ' ', text_data)\n","\n","        processed_article = re.sub('[^a-zA-Z]', ' ', text_data)\n","        processed_article = re.sub(r'\\s+', ' ', processed_article)\n","\n","        # sentence-level tokenization of full text\n","        sentence_list = nltk.sent_tokenize(text_data)\n","\n","        # NLTK stopword list\n","        stopwords = nltk.corpus.stopwords.words('english')\n","\n","        # creating term frequency dict\n","        word_frequencies = {}\n","        for word in nltk.word_tokenize(processed_article):\n","            if word not in stopwords:\n","                if word not in word_frequencies.keys():\n","                    word_frequencies[word] = 1\n","                else:\n","                    word_frequencies[word] += 1\n","\n","        maximum_frequency = max(word_frequencies.values())\n","\n","        # adding term frequency ratios as dict values\n","        for word in word_frequencies.keys():\n","            word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n","\n","        # ranking sentences for summary inclusion\n","        sentence_scores = {}\n","        for sent in sentence_list:\n","            for word in nltk.word_tokenize(sent.lower()):\n","                if word in word_frequencies.keys():\n","                    if len(sent.split(' ')) < 30:\n","                        if sent not in sentence_scores.keys():\n","                            sentence_scores[sent] = word_frequencies[word]\n","                        else:\n","                            sentence_scores[sent] += word_frequencies[word]\n","\n","        # creating final summary with default 4 highest-scoring sentences\n","        summary_sentences = heapq.nlargest(\n","            4, sentence_scores, key=sentence_scores.get)\n","        summary_sentences = ''.join(summary_sentences)\n","        return summary_sentences\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"jR4wALy8CwTB"},"execution_count":null,"outputs":[]}]}