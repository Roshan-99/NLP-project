{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UBhj4GbC_WnN"
      },
      "outputs": [],
      "source": [
        "# summarize.py\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"summarize.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1y4HxABEY_nyJrfR-6zG015fyu-Z0zakJ\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import heapq\n",
        "import logging\n",
        "import nltk\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "from summarizer.model_processors import SingleModel\n",
        "\n",
        "class ModelSelector(object):\n",
        "    min_length = 40\n",
        "    # init method or constructor\n",
        "    def __init__(self):\n",
        "    # choose BERT or vanilla summarizer\n",
        "        choose_bert_or_vanilla = '1'\n",
        "        # choose_bert_or_vanilla = input('Please enter 1 to use the BERT summarizer or 2 for the Vanilla summarizer:\\n')\n",
        "\n",
        "        # BERT summarizer\n",
        "        if choose_bert_or_vanilla == '1':\n",
        "            print('Welcome to the BERT Summarizer!\\n')\n",
        "            self.process_documents(\"bert\")\n",
        "            print('Processing finished...!')\n",
        "\n",
        "        # Vanilla summarizer\n",
        "        elif choose_bert_or_vanilla == '2':\n",
        "            print('Welcome to the Vanilla Summarizer!\\n')\n",
        "            self.process_documents(\"vanilla\")\n",
        "            print('Processing finished...!')\n",
        "        else:\n",
        "            print('\\nMust choose from 1 or 2')\n",
        "\n",
        "    def process_documents(self,model_type):\n",
        "        path = os.getcwd()+'/../Data/summarized_data/content'\n",
        "        categories = os.listdir(path)\n",
        "        for category in categories:\n",
        "            category_path = path+'/'+category\n",
        "            # Check whether a path pointing to a file\n",
        "            if os.path.isfile(category_path) == False:\n",
        "                documents = os.listdir(category_path)\n",
        "                for document in documents:\n",
        "                    print('Doc ', document)\n",
        "                    self.summarize_text(model_type, document, category_path)\n",
        "\n",
        "\n",
        "    def summarize_text(self, model_type, doc, path):\n",
        "        # print(model,doc,path)\n",
        "        document = path+'/'+doc\n",
        "        # reading in text file\n",
        "\n",
        "        with open(document, 'r') as d:\n",
        "            text_data = d.read()\n",
        "        # print('text:',path,len(text_data))\n",
        "        \n",
        "        if os.path.getsize(document) <self.min_length or len(text_data)<self.min_length:\n",
        "            summary = \"File was empty!\"\n",
        "        else:\n",
        "            # Passing full text to model\n",
        "            if model_type == \"bert\":\n",
        "                model = SingleModel()\n",
        "                summary = model(text_data)\n",
        "            else:\n",
        "                summary = self.vanilla(text_data)\n",
        "\n",
        "\n",
        "        # creating final summary with a ratio of 0.13\n",
        "        summary_file = '\\n\\nSUMMARY:\\n' + summary\n",
        "\n",
        "        folders = document.split('/')\n",
        "        filepath = folders[-2]+'/'+folders[-1]\n",
        "        write_path = os.getcwd()+'/../Data/summarized_data/summaries/'+filepath\n",
        "        with open(write_path, 'w+') as summary_output:\n",
        "            for line in summary_file:\n",
        "                summary_output.write(line)\n",
        "\n",
        "\n",
        "    def vanilla(self,text_data):\n",
        "\n",
        "        # text clean up\n",
        "        text_data = re.sub(r'\\[[0-9]*\\]', ' ', text_data)\n",
        "        text_data = re.sub(r'\\s+', ' ', text_data)\n",
        "\n",
        "        processed_article = re.sub('[^a-zA-Z]', ' ', text_data)\n",
        "        processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
        "\n",
        "        # sentence-level tokenization of full text\n",
        "        sentence_list = nltk.sent_tokenize(text_data)\n",
        "\n",
        "        # NLTK stopword list\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "        # creating term frequency dict\n",
        "        word_frequencies = {}\n",
        "        for word in nltk.word_tokenize(processed_article):\n",
        "            if word not in stopwords:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "        maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "        # adding term frequency ratios as dict values\n",
        "        for word in word_frequencies.keys():\n",
        "            word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
        "\n",
        "        # ranking sentences for summary inclusion\n",
        "        sentence_scores = {}\n",
        "        for sent in sentence_list:\n",
        "            for word in nltk.word_tokenize(sent.lower()):\n",
        "                if word in word_frequencies.keys():\n",
        "                    if len(sent.split(' ')) < 30:\n",
        "                        if sent not in sentence_scores.keys():\n",
        "                            sentence_scores[sent] = word_frequencies[word]\n",
        "                        else:\n",
        "                            sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "        # creating final summary with default 4 highest-scoring sentences\n",
        "        summary_sentences = heapq.nlargest(\n",
        "            4, sentence_scores, key=sentence_scores.get)\n",
        "        summary_sentences = ''.join(summary_sentences)\n",
        "        return summary_sentences\n",
        "\n",
        "# ModelSelector()"
      ]
    }
  ]
}