{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b30e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import fitz\n",
    "import json\n",
    "import re \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PURPOSE_GOAL='What is the goal or aim or purpose of the project'\n",
    "DESCRIPTION_OVERVIEW='What is the description or overview of the system'\n",
    "ASSUMPTIONS_DEPENDENCIES='What are the assumptions and dependencies'\n",
    "SCOPE='What is the scope'\n",
    "REQUIREMENTS_FUNCTIONAL_NON= 'What are the requirements functional non functional'\n",
    "SYSTEM_ARCHITECTURE='What is the system architecture or what is the architecture of the software'\n",
    "USERS_AUDIENCE='Who are the users or audience'\n",
    "QUESTIONS_LIST=[PURPOSE_GOAL,DESCRIPTION_OVERVIEW,ASSUMPTIONS_DEPENDENCIES, SCOPE, REQUIREMENTS_FUNCTIONAL_NON, SYSTEM_ARCHITECTURE, USERS_AUDIENCE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonts(doc, granularity=False):\n",
    "    \"\"\"Extracts fonts and their usage in PDF documents.\n",
    "    :param doc: PDF document to iterate through\n",
    "    :type doc: <class 'fitz.fitz.Document'>\n",
    "    :param granularity: also use 'font', 'flags' and 'color' to discriminate text\n",
    "    :type granularity: bool\n",
    "    :rtype: [(font_size, count), (font_size, count}], dict\n",
    "    :return: most used fonts sorted by count, font style information\n",
    "    \"\"\"\n",
    "    styles = {}\n",
    "    font_counts = {}\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # block contains text\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        if granularity:\n",
    "                            identifier = \"{0}_{1}_{2}_{3}\".format(s['size'], s['flags'], s['font'], s['color'])\n",
    "                            styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],\n",
    "                                                  'color': s['color']}\n",
    "                        else:\n",
    "                            identifier = \"{0}\".format(s['size'])\n",
    "                            styles[identifier] = {'size': s['size'], 'font': s['font']}\n",
    "\n",
    "                        font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage\n",
    "\n",
    "    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if len(font_counts) < 1:\n",
    "        raise ValueError(\"Zero discriminating fonts found!\")\n",
    "\n",
    "    return font_counts, styles\n",
    "\n",
    "\n",
    "def font_tags(font_counts, styles):\n",
    "    \"\"\"Returns dictionary with font sizes as keys and tags as value.\n",
    "    :param font_counts: (font_size, count) for all fonts occuring in document\n",
    "    :type font_counts: list\n",
    "    :param styles: all styles found in the document\n",
    "    :type styles: dict\n",
    "    :rtype: dict\n",
    "    :return: all element tags based on font-sizes\n",
    "    \"\"\"\n",
    "    p_style = styles[font_counts[0][0]]  # get style for most used font by count (paragraph)\n",
    "    p_size = p_style['size']  # get the paragraph's size\n",
    "\n",
    "    # sorting the font sizes high to low, so that we can append the right integer to each tag\n",
    "    font_sizes = []\n",
    "    for (font_size, count) in font_counts:\n",
    "        font_sizes.append(float(font_size))\n",
    "    font_sizes.sort(reverse=True)\n",
    "\n",
    "    # aggregating the tags for each font size\n",
    "    idx = 0\n",
    "    size_tag = {}\n",
    "    for size in font_sizes:\n",
    "        idx += 1\n",
    "        if size == p_size:\n",
    "            idx = 0\n",
    "            size_tag[size] = '<p>'\n",
    "        if size > p_size:\n",
    "            size_tag[size] = '<h{0}>'.format(idx)\n",
    "        elif size < p_size:\n",
    "            size_tag[size] = '<s{0}>'.format(idx)\n",
    "\n",
    "    return size_tag\n",
    "\n",
    "\n",
    "def headers_para(doc, size_tag):\n",
    "    \"\"\"Scrapes headers & paragraphs from PDF and return texts with element tags.\n",
    "    :param doc: PDF document to iterate through\n",
    "    :type doc: <class 'fitz.fitz.Document'>\n",
    "    :param size_tag: textual element tags for each size\n",
    "    :type size_tag: dict\n",
    "    :rtype: list\n",
    "    :return: texts with pre-prended element tags\n",
    "    \"\"\"\n",
    "    header_para = []  # list with headers and paragraphs\n",
    "    first = True  # boolean operator for first header\n",
    "    previous_s = {}  # previous span\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # this block contains text\n",
    "\n",
    "                # REMEMBER: multiple fonts and sizes are possible IN one block\n",
    "\n",
    "                block_string = \"\"  # text found in block\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        if s['text'].strip():  # removing whitespaces:\n",
    "                            if first:\n",
    "                                previous_s = s\n",
    "                                first = False\n",
    "                                block_string = size_tag[s['size']] + s['text']\n",
    "                            else:\n",
    "                                if s['size'] == previous_s['size']:\n",
    "\n",
    "                                    if block_string and all((c == \"|\") for c in block_string):\n",
    "                                        # block_string only contains pipes\n",
    "                                        block_string = size_tag[s['size']] + s['text']\n",
    "                                    if block_string == \"\":\n",
    "                                        # new block has started, so append size tag\n",
    "                                        block_string = size_tag[s['size']] + s['text']\n",
    "                                    else:  # in the same block, so concatenate strings\n",
    "                                        block_string += \" \" + s['text']\n",
    "\n",
    "                                else:\n",
    "                                    header_para.append(block_string)\n",
    "                                    block_string = size_tag[s['size']] + s['text']\n",
    "\n",
    "                                previous_s = s\n",
    "\n",
    "                    # new block started, indicating with a pipe\n",
    "                    block_string += \"|\"\n",
    "\n",
    "                header_para.append(block_string)\n",
    "\n",
    "    return header_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_creator(list_sentences,n_gram):\n",
    "    paras=[]\n",
    "    start,end=0,n_gram\n",
    "    while( end<len(list_sentences)):\n",
    "        combined=list_sentences[start:end]\n",
    "        start+=1\n",
    "        end+=1\n",
    "        paras.append(' '.join(combined))\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tags(sentence):\n",
    "   \n",
    "    sentence=re.sub('<h[0-9]>|<s[0-9]*>',\"\",sentence)\n",
    "    sentence=re.sub(' +', ' ',sentence)\n",
    "    sentence=sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d246ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_the_paras(filepath,n_grams):\n",
    "    document = filepath\n",
    "    doc = fitz.open(document)\n",
    "\n",
    "    font_counts, styles = fonts(doc, granularity=False)\n",
    "\n",
    "    size_tag = font_tags(font_counts, styles)\n",
    "\n",
    "    elements = headers_para(doc, size_tag)\n",
    "\n",
    "    # with open(\"doc.json\", 'w') as json_out:\n",
    "    #     json.dump(elements, json_out)\n",
    "    tagged_list=(\" \".join(elements).split(\"|\"))\n",
    "    Soup = BeautifulSoup(\" \".join(elements), 'lxml')\n",
    "    # heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "    heading_tags = [\"h3\"]\n",
    "    #for tags in Soup.find_all(heading_tags):\n",
    "        #print(tags.name + ' -> ' + tags.text.strip())\n",
    "    paras=[]\n",
    "    for tag_sentence in tagged_list:\n",
    "        transformed_temp=transform_tags(tag_sentence)\n",
    "        if len(transformed_temp)==0:\n",
    "            continue\n",
    "        paras.append(transformed_temp)\n",
    "    splitted_paras=(' '.join(paras)).split('<p>')\n",
    "    corpus=n_combined_grams(splitted_paras,n_grams)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_combined_grams(splitted_paras,list_grams):\n",
    "    all_grams=[]\n",
    "    for n_gram in list_grams:\n",
    "        all_grams.append(n_gram_creator(splitted_paras,n_gram))\n",
    "    flat_list = [item for sublist in all_grams for item in sublist]\n",
    "    return flat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ad926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(corpus)\n",
    "top_list=bm25.get_top_n(\"purpose of the document\".split(\" \"),corpus, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank,val in enumerate(top_list):\n",
    "    print(rank+1,':',val,\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052821c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rank,val in enumerate(top_list):\n",
    "    print(rank+1,':',val,\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe851f",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text2(vectorizer, text, verbose=False):\n",
    "    '''\n",
    "    Transform the text in a vector[Word2Vec]\n",
    "    vectorizer: sklearn.vectorizer\n",
    "    text: str\n",
    "    '''\n",
    "    tokens = preprocess_string(text)\n",
    "    words = [vectorizer[w] for w in tokens if w in vectorizer]\n",
    "    if verbose:\n",
    "        print('Text:', text)\n",
    "        print('Vector:', [w for w in tokens if w in vectorizer])\n",
    "    elif len(words):\n",
    "        return np.mean(words, axis=0)\n",
    "    else:\n",
    "        return np.zeros((300), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_para_summarizer(corpus,questions_list):\n",
    "    corpus_df =  pd.DataFrame({'paras':corpus})\n",
    "    corpus_df['id_'] = range(0, len(corpus_df))\n",
    "    \n",
    "    corpus_list = corpus_df['paras'].tolist()\n",
    "    corpus_token = [preprocess_string(t) for t in corpus_list]\n",
    "    vectorizer = Word2Vec(sentences=corpus_token, vector_size=300, window=5, min_count=1, workers=4).wv\n",
    "    retriever_configs = {\n",
    "    'n_neighbors': 5,\n",
    "    'metric': 'cosine'\n",
    "    }\n",
    "    retriever = NearestNeighbors(**retriever_configs)\n",
    "\n",
    "    # vectorizer the documents, fit the retriever\n",
    "    X = corpus_df['paras'].apply(lambda x: transform_text2(vectorizer, x)).tolist()\n",
    "    retriever.fit(X, corpus_df['id_'])\n",
    "    \n",
    "    #print(questions_list)\n",
    "    #questions_list=['What is the goal or aim or purpose of the project', 'What is the description or overview of the system','What are the assumptions and dependencies', 'What is the scope', 'What are the requirements functional non functional', 'What is the system architecture or what is the architecture of the software', 'Who are the users or audience']\n",
    "    X=[transform_text2(vectorizer,question) for question in questions_list]\n",
    "    #X = questions_list.apply(lambda x: transform_text2(vectorizer, x))\n",
    "    # y_test = data['c_id']\n",
    "    y_pred = retriever.kneighbors(X, return_distance=False)\n",
    "    json_output={}\n",
    "    for question,index in enumerate(y_pred):\n",
    "        \n",
    "        #print(questions_list[question],\"?:\")\n",
    "        inner_json={}\n",
    "        for rank,i_ in enumerate(index):\n",
    "            inner_json[rank+1]=corpus_df.iloc[i_,0]\n",
    "            #print(rank+1,\":\",corpus_df.iloc[i_,0],\"\\n\")\n",
    "        json_output[questions_list[question]]=inner_json\n",
    "        #print(\"\\n\")\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c6c3e",
   "metadata": {},
   "source": [
    "## Word2Vec results on all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=r\"../../Data/SRS Dataset Clean/\"\n",
    "files=os.listdir(folder_path)\n",
    "output_dict=dict()\n",
    "for file in files:\n",
    "    \n",
    "    if '.pdf' not in file:\n",
    "        continue\n",
    "    filepath=folder_path+file\n",
    "    #print(filepath)\n",
    "    filepath_result={}\n",
    "    try:\n",
    "        corpus=make_the_paras(filepath,[3])\n",
    "        \n",
    "        filepath_result=word2vec_para_summarizer(corpus,QUESTIONS_LIST)\n",
    "    except:\n",
    "        print(filepath)\n",
    "    finally:\n",
    "        output_dict[filepath]=filepath_result\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "out_file = open(\"word2vec_output.json\", \"w\")\n",
    "  \n",
    "json.dump(output_dict, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_para_summarizer(make_the_paras(r\"../../Data/SRS Dataset Clean/SRS20_removed.pdf\",[3]),QUESTIONS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47096ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_the_paras(r\"../../Data/SRS Dataset Clean/SRS20_removed.pdf\",[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da65e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ranks_output=dict()\n",
    "for filepath in output_dict:\n",
    "    modified_filepath=((filepath.split('/')[-1]).split('.')[0]).split('_')[0]\n",
    "    \n",
    "    combined_ranks_output[filepath]=dict()\n",
    "    \n",
    "    for question in output_dict[filepath]:\n",
    "        #combined_ranks_output[filepath][question]=\n",
    "        #combined_ranks_output[filepath][question]\n",
    "        all_paras=''\n",
    "        for rank in output_dict[filepath][question]:\n",
    "            all_paras+=output_dict[filepath][question][rank]\n",
    "        combined_ranks_output[filepath][question]=all_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82213f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS=['purpose','description','scope','requirements','assumptions and dependencies','architecture','users']\n",
    "output_dataframe=pd.DataFrame.from_dict(combined_ranks_output,orient='index')\n",
    "output_dataframe.columns=CSV_COLUMNS\n",
    "display(output_dataframe)\n",
    "output_dataframe.to_csv('labelled_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ranks_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('labelled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ab924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
